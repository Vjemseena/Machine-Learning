{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree for Classification & Regression\n",
    "\n",
    "A decision tree is essentially a series of if-then statements, that, when applied to a record in a data set, results in the classification of that record. Therefore, once you've created your decision tree, you will be able to run a data set through the program and get a classification for each individual record within the data set. What this means to you, as a manufacturer of quality widgets, is that the program you create from this article will be able to predict the likelihood of each user, within a data set, purchasing your finely crafted product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator in input variables.\n",
    "\n",
    "<img src=\"images/2.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How decision tree works\n",
    "\n",
    "\n",
    "The understanding level of Decision Trees algorithm is so easy compared with other classification algorithms. The decision tree algorithm tries to solve the problem, by using tree representation. Each internal node of the tree corresponds to an attribute, and each leaf node corresponds to a class label.\n",
    "\n",
    "## Decision Tree Algorithm Pseudocode\n",
    "\n",
    "   * Place the best attribute of the dataset at the root of the tree.\n",
    "   * Split the training set into subsets. Subsets should be made in such a way that each subset contains data with the same value for an attribute.\n",
    "   * Repeat step 1 and step 2 on each subset until you find leaf nodes in all the branches of the tree.\n",
    " \n",
    "\n",
    "### Decision Tree classifier\n",
    "In decision trees, for predicting a class label for a record we start from the root of the tree. We compare the values of the root attribute with record’s attribute. On the basis of comparison, we follow the branch corresponding to that value and jump to the next node.\n",
    "\n",
    "We continue comparing our record’s attribute values with other internal nodes of the tree until we reach a leaf node with predicted class value. As we know how the modeled decision tree can be used to predict the target class or the value. Now let’s understanding how we can create the decision tree model.\n",
    "\n",
    "### Assumptions while creating Decision Tree\n",
    "\n",
    "The below are the some of the assumptions we make while using Decision tree:\n",
    "\n",
    "   * At the beginning, the whole training set is considered as the root.\n",
    "   * Feature values are preferred to be categorical. If the values are continuous then they are discretized prior to building the model.\n",
    "   * Records are distributed recursively on the basis of attribute values.\n",
    "   * Order to placing attributes as root or internal node of the tree is done by using some statistical approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to split nodes\n",
    "\n",
    "There are few algorithms to find optimum split. Let's look at the following to understand the mathematics behind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "An alternative splitting criterion for decision tree learning algorithms is *information gain*. It measures how well a particular attribute distinguishes among different target classifications. Information gain is measured in terms of the expected reduction in the entropy or impurity of the data. The entropy of a set of probabilities is:\n",
    "\n",
    "$$H(p) = -\\sum_i p_i log_2(p_i)$$\n",
    "\n",
    "If we have a set of binary responses from some variable, all of which are positive/true/1, then knowing the values of the variable does not hold any predictive value for us, since all the outcomes are positive. Hence, the entropy is zero:\n",
    "\n",
    "<img src=\"images/ent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy calculation tells us how much additional information we would obtain with knowledge of the variable.\n",
    "\n",
    "So, if we have a set of candidate covariates from which to choose as a node in a decision tree, we should choose the one that gives us the most information about the response variable (*i.e.* the one with the highest entropy).\n",
    "\n",
    "### Misclassification Rate\n",
    "\n",
    "Alternatively, we can use the misclassification rate:\n",
    "\n",
    "$$C(j,t) = \\frac{1}{n_{jt}} \\sum_{y_i: x_{ij} \\gt t} I(y_i \\ne \\hat{y})$$\n",
    "\n",
    "where $\\hat{y}$ is the most probable class label and $n_{ij}$ is the number of observations in the data subset obtained from splitting via $j,t$.\n",
    "\n",
    "### Gini index\n",
    "\n",
    "The Gini index is simply the expected error rate:\n",
    "\n",
    "$$C(j,t) = \\sum_{k=1}^K \\hat{\\pi}_{jt}[k] (1 - \\hat{\\pi}_{jt}[k]) = 1 - \\sum_{k=1}^K \\hat{\\pi}_{jt}[k]^2$$\n",
    "\n",
    "where $\\hat{\\pi}_{jt}[k]$ is the probability of an observation being correctly classified as class $k$ for the data subset obtained from splitting via $j,t$ (hence, $(1 - \\hat{\\pi}_{jt}[k])$ is the misclassification probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, pydotplus\n",
    "from sklearn import tree, metrics, model_selection, preprocessing\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                                                               Heart Disease Case Study\n",
    "\n",
    " Objective : Predict the presence of heart disease in a pateint. \n",
    "\n",
    " Data contain\n",
    " a binary outcome HD for 454  patients who presented with chest pain.\n",
    " An outcome value of Yes indicates the presence of heart disease based on\n",
    " an angiographic test, while No means no heart disease\n",
    "\n",
    " \n",
    " There are 13 predictors\n",
    " including Age, Sex, Chol (a cholesterol measurement), and other heart\n",
    " and lung function measurements\n",
    "\n",
    " In the data, some of the predictors, such as Sex, Thal (Thallium stress test),\n",
    " and ChestPain, are qualitative\n",
    "\n",
    "\n",
    " Data Source-UCI Machine Learning Repository\n",
    "\n",
    " Data Description\n",
    "\n",
    " \n",
    " 1. (age)-> age in years\n",
    " 2. (sex)-sex (1 = male; 0 = female) \n",
    " 3. (chestPain)->-- Value 1: typical angina\n",
    " -- Value 2: atypical angina\n",
    " -- Value 3: non-anginal pain\n",
    " -- Value 4: asymptomatic \n",
    " 4.  (RestBP)->resting blood pressure (in mm Hg on admission to the hospital) \n",
    " 5.  (chol)->serum cholestoral in mg/dl \n",
    " 6.  (fbs)->(fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n",
    " 7.  (restecg)->resting electrocardiographic results \n",
    " -- Value 0: normal\n",
    " -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    " -- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria \n",
    " 8. (MaxHR)->maximum heart rate achieved \n",
    " 9.  (exang)->exercise induced angina (1 = yes; 0 = no) \n",
    " 10. (oldpeak)->ST depression induced by exercise relative to rest \n",
    " 11. (slope)->the slope of the peak exercise ST segment\n",
    " -- Value 1: upsloping\n",
    " -- Value 2: flat\n",
    " -- Value 3: downsloping \n",
    " 12. (ca)->number of major vessels (0-3) colored by flourosopy \n",
    " 13. (thal)->3 = normal; 6 = fixed defect; 7 = reversable defect \n",
    " 14. (AHD) (the predicted attribute) ->diagnosis of heart disease (angiographic disease status) Yes/No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('trainData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AHD']=df['AHD'].apply(lambda x : 1 if x=='Yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['AHD'],_=pd.factorize(df['AHD'],sort=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LabelEncoder and initializing it\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(df['ChestPain'])\n",
    "df['ChestPain']=pd.Categorical(le.transform(df['ChestPain']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(df['Thal'])\n",
    "df['Thal']=pd.Categorical(le.transform(df['Thal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features\n",
    "y_train = df['AHD']\n",
    "X_train = df.iloc[:,0:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the decision tree\n",
    "dtree = tree.DecisionTreeClassifier(criterion='gini',min_samples_leaf=10,max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(dtree, \n",
    "                   feature_names=X_train.columns,  \n",
    "                   class_names=['0','1'],\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?dtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?dtree.predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_prob = dtree.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?y_train_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_train_pred_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_prob[:5,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_class= y_train_pred_prob[:,1]>0.5\n",
    "y_train_pred_class[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_train,y_train_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(128+149)/317"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = dtree.predict(X_train)\n",
    "y_train_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_train,y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on Test Data Set\n",
    "# load the data\n",
    "df_test = pd.read_csv('testData.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.factorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['AHD']=df_test['AHD'].apply(lambda x : 1 if x=='Yes' else 0)\n",
    "#df_test['AHD'],_=pd.factorize(df_test['AHD'],sort=False)\n",
    "\n",
    "le.fit(df_test['ChestPain'])\n",
    "df_test['ChestPain']=pd.Categorical(le.transform(df_test['ChestPain']))\n",
    "\n",
    "le.fit(df_test['Thal'])\n",
    "df_test['Thal']=pd.Categorical(le.transform(df_test['Thal']))\n",
    "df_test=df_test.dropna()\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features\n",
    "y_test = df_test['AHD']\n",
    "X_test = df_test.iloc[:,0:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_pred_prob = dtree.predict_proba(X_test)\n",
    "y_test_pred_prob[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test,y_test_pred_prob[:,0]>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descision Tree excercise on IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris data\n",
    "df = pd.read_csv('iris.csv')\n",
    "df['species_label'], _ = pd.factorize(df['species'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features\n",
    "y = df['species_label']\n",
    "X = df[['petal_length', 'petal_width']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data randomly into 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and make predictions\n",
    "\n",
    "Note we didn't have to standardize the data to use a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the decision tree\n",
    "dtree = tree.DecisionTreeClassifier(criterion='gini',random_state=0)\n",
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = dtree.predict_proba(X_test)\n",
    "y_pred_prob[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions with the test data\n",
    "y_pred = dtree.predict(X_test)\n",
    "y_pred[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model's performance\n",
    "\n",
    "Including the tree's axis-parallel decision boundaries and how the tree splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did our model perform?\n",
    "count_misclassified = (y_test != y_pred).sum()\n",
    "print('Misclassified samples: {}'.format(count_misclassified))\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "43/45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "For visualizing decision tree splits I am creating **plot_decision()** function below using matplotlib. If you dont understand the implementation completely that's fine. It is just for the understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def plot_decision(X, y, classifier, test_idx=None, resolution=0.02, figsize=(8,8)):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('#cc0000', '#003399', '#00cc00', '#999999', '#66ffff')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # get dimensions\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n",
    "    xmin = xx1.min()\n",
    "    xmax = xx1.max()\n",
    "    ymin = xx2.min()\n",
    "    ymax = xx2.max()\n",
    "    \n",
    "    # create the figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    \n",
    "    # plot the decision surface\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    ax.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap, zorder=1)\n",
    "    \n",
    "    # plot all samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        ax.scatter(x=X[y == cl, 0], \n",
    "                   y=X[y == cl, 1],\n",
    "                   alpha=0.6, \n",
    "                   c=cmap(idx),\n",
    "                   edgecolor='black',\n",
    "                   marker='o',#markers[idx],\n",
    "                   s=50,\n",
    "                   label=cl,\n",
    "                   zorder=3)\n",
    "\n",
    "    # highlight test samples\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        ax.scatter(X_test[:, 0],\n",
    "                   X_test[:, 1],\n",
    "                   c='w',\n",
    "                   alpha=1.0,\n",
    "                   edgecolor='black',\n",
    "                   linewidths=1,\n",
    "                   marker='o',\n",
    "                   s=150, \n",
    "                   label='test set',\n",
    "                   zorder=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the model's decision regions to see how it separates the samples\n",
    "X_combined = np.vstack((X_train, X_test))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "plot_decision(X=X_combined, y=y_combined, classifier=dtree)\n",
    "plt.xlabel('petal length')\n",
    "plt.ylabel('petal width')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing, but this time identify the points that constituted the test data set\n",
    "test_idx = range(len(y_train), len(y_combined))\n",
    "plot_decision(X=X_combined, y=y_combined, classifier=dtree, test_idx=test_idx)\n",
    "plt.xlabel('petal length')\n",
    "plt.ylabel('petal width')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "Overfitting is a practical problem while building a decision tree model. The model is having an issue of overfitting is considered when the algorithm continues to go deeper and deeper in the to reduce the training set error but results with an increased test set error i.e, Accuracy of prediction for our model goes down. It generally happens when it builds many branches due to outliers and irregularities in data.\n",
    "\n",
    "#### Two approaches which we can use to avoid overfitting are:\n",
    "\n",
    "   * Pre-Pruning\n",
    "   * Post-Pruning\n",
    "\n",
    "\n",
    "##### Pre-Pruning\n",
    "\n",
    "In pre-pruning, it stops the tree construction bit early. It is preferred not to split a node if its goodness measure is below a threshold value. But it’s difficult to choose an appropriate stopping point.\n",
    "\n",
    "##### Post-Pruning\n",
    "\n",
    "In post-pruning first, it goes deeper and deeper in the tree to build a complete tree. If the tree shows the overfitting problem then pruning is done as a post-pruning step. We use a cross-validation data to check the effect of our pruning. Using cross-validation data, it tests whether expanding a node will make an improvement or not.\n",
    "\n",
    "If it shows an improvement, then we can continue by expanding that node. But if it shows a reduction in accuracy then it should not be expanded i.e, the node should be converted to a leaf node.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/pruning.png\">\n",
    "\n",
    "### Decision Tree Algorithm Advantages and Disadvantages\n",
    "\n",
    "##### Advantages:\n",
    "\n",
    "   * Decision Trees are easy to explain. It results in a set of rules.\n",
    "   * It follows the same approach as humans generally follow while making decisions.\n",
    "   * Interpretation of a complex Decision Tree model can be simplified by its visualizations. Even a naive person can understand logic.\n",
    "   * The Number of hyper-parameters to be tuned is almost null.\n",
    "\n",
    "\n",
    "##### Disadvantages:\n",
    "\n",
    "   * There is a high probability of overfitting in Decision Tree.\n",
    "   * Generally, it gives low prediction accuracy for a dataset as compared to other machine learning algorithms.\n",
    "   * Information gain in a decision tree with categorical variables gives a biased response for attributes with greater no. of categories.\n",
    "   * Calculations can become complex when there are many class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree for Regression Problem- ie. when the target variable is continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "dataset = load_boston()\n",
    "X = dataset.data\n",
    "Y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame(X)\n",
    "X\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=pd.DataFrame(Y)\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "bag_tree = DecisionTreeRegressor(max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_tree.fit(X_train, y_train)\n",
    "bag_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(bag_tree,feature_names=X_train.columns,filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Material\n",
    "* https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1\n",
    "\n",
    "* http://www.r2d3.us/visual-intro-to-machine-learning-part-1/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
